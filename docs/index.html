<!doctype html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-138229553-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-138229553-1');
    </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <title>rGAN</title>
  </head>

  <style type="text/css">
    footer {
    padding-top: 10px;
    padding-bottom: 10px;
    }
    .bg-whitesmoke {
    background-color: whitesmoke
    }
    .thumbnail-shadow {
    filter: drop-shadow(5px 5px 5px #aaa);
    }
    table{
    margin: 0 auto
    }
  </style>
  
  <body>
    <header>
      <div class="jumbotron text-center bg-whitesmoke">
	<div class="container">
	  <h2>Label-Noise Robust Generative Adversarial Networks</h2>
	  <p class="lead">
	    <a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/">Takuhiro Kaneko</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
	    <a href="https://yoshitakaushiku.net/">Yoshitaka Ushiku</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
	    <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada</a><sup>1,2</sup>&nbsp;&nbsp;&nbsp;<br>
	    <sup>1</sup>The University of Tokyo&nbsp;&nbsp;&nbsp;
	    <sup>2</sup>RIKEN
	  </p>
	  <p class="lead">
	  CVPR 2019 (Oral)<br>
	  <a href="https://arxiv.org/abs/1811.11165">[Paper]</a>
	  <a href="https://github.com/takuhirok/rGAN/">[Code]</a>
	  <a href="rGAN_slides.pdf">[Slides]</a>
	  <a href="rGAN_poster.pdf">[Poster]</a>
	  <a href="https://www.youtube.com/watch?v=9GR8V-VR4Qg&t=5300s">[Talk]</a>
	  </p>
	</div>
      </div>
    </header>

    <main>
      <div class="container">
	<figure class="figure text-center">
	  <p>
	    <a name="fig1"><img class="w-100" src="images/examples.png" alt="examples"></a>
	  </p>
	  <figcaption class="figure-caption text-left">
	    Figure 1. Examples of label-noise robust conditional image generation. rGAN can learn a label-noise robust conditional generator that can generate an image conditioned on the <em>clean label</em> even when the <em>noisy labeled images</em> are only available for training.
	  </figcaption>
	</figure>

	<p>
	  <strong>Note:</strong> In our other studies, we have also proposed GAN for <em>ambiguous labels</em>, GAN for <em>image noise</em>, and GAN for <em>blur, noise, and compression</em>. Please check them from the links below.
	</p>
	<p class="text-center">
          <a href="https://takuhirok.github.io/CP-GAN/"><strong>Classifier's posterior GAN (CP-GAN)</strong></a> (BMVC 2019):
	  GAN for <em>ambiguous labels</em><br>
	  <a href="https://takuhirok.github.io/NR-GAN/"><strong>Noise robust GAN (NR-GAN)</strong></a> (CVPR 2020):
	  GAN for <em>image noise</em><br>
	  <a href="https://takuhirok.github.io/BNCR-GAN/"><strong>Blur, noise, and compression robust GAN (BNCR-GAN)</strong></a> (CVPR 2021):
	  GAN for <em>blur, noise, and compression</em>
	</p>

	<h3 class="text-center">Abstract</h3>
	<p>
	  Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class-conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called <strong>label-noise robust GANs (rGANs)</strong>, which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: <strong>rAC-GAN</strong>, which is a bridging model between AC-GAN and the label-noise robust classification model, and <strong>rcGAN</strong>, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).
	</p>
	
	<h3 class="text-center">Paper</h3>
	<p>
	  <table>
	    <tbody>
	      <tr>
		<td>
		  <a href="https://arxiv.org/abs/1811.11165"><img class="thumbnail-shadow" alt="paper thumbnail" src="images/paper_thumbnail.png"  width=150></a>
		</td>
		<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
		<td class="text-center">
		  <p>
		    <a href="https://arxiv.org/abs/1811.11165" class="lead">[Paper]</a><br>
		    arXiv:1811.11165<br>Nov. 2018.
		  </p>
		  <p>
		    <a href="rGAN_slides.pdf" class="lead">[Slides]</a>
		    <a href="rGAN_poster.pdf" class="lead">[Poster]</a>
		    <a href="https://www.youtube.com/watch?v=9GR8V-VR4Qg&t=5300s" class="lead">[Talk]</a>
		  </p>		  
		</td>
	      </tr>
	    </tbody>
	  </table>
	</p>

	<h5 class="text-center">Citation</h5>
	<p class="text-center">
	  Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada.<br>
	  Label-Noise Robust Generative Adversarial Networks.
	  In CVPR, 2019.<br>
	  <a href="rGAN.txt" class="lead">[BibTex]</a>
	</p>
	
	<h3 class="text-center">Code</h3>
	<p class="text-center lead">
	  <a href="https://github.com/takuhirok/rGAN/">[PyTorch]</a>
	</p>

	<h3 class="text-center">Talk</h3>
	<p class="text-center lead">
	  <iframe width="800" height="450" src="https://www.youtube.com/embed/9GR8V-VR4Qg?start=5300" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</p>
	
	<h3 class="text-center">Overview</h3>
	<p>
	  Our task is, when given <em>noisy labeled</em> data, to construct a label-noise robust conditional generator that can generate an image conditioned on the <em>clean label</em> rather than conditioned on the <em>noisy label</em>. Our main idea for solving this problem is to incorporate a <strong>noise transition model</strong> (viewed as orange rectangles in Figures 2(b) and (d); which represents a probability that a clean label is corrupted to a noisy label) into typical class-conditional GANs. In particular, we develop two variants: <strong>rAC-GAN</strong> (<a href="#fig2">Figure 2(b)</a>) and <strong>rcGAN</strong> (<a href="#fig2">Figure 2(d)</a>) that are extensions of AC-GAN <a href="#ref1">[1]</a> (<a href="#fig2">Figure 2(a)</a>) and cGAN <a href="#ref2">[2]</a> <a href="#ref3">[3]</a> (<a href="#fig2">Figure 2(c)</a>), respectively.
	</p>
	
	<figure class="figure text-center">
	  <p>
	    <a name="fig2"><img class="w-100" src="images/networks.png" alt="examples"></a>
	  </p>
	  <figcaption class="figure-caption text-left">
	    Figure 2. Comparison of standard conditional GANs and label-noise robust GANs. We denote the discriminator and auxiliary classifier by <em>D</em> and <em>C</em>, respectively. In our rAC-GAN (b) and rcGAN (d), we incorporate a noise transition model (viewed as an orange rectangle) into AC-GAN (a) and cGAN (c), respectively.
	  </figcaption>
	</figure>

	<h3 class="text-center">Examples of generated images</h3>
	<h5 class="text-center">CIFAR-10 (symmetric noise with a noise rate of 0.5)</h5>
	<figure class="figure text-center">
	  <p>
	    <a name="fig3-1"><img class="w-75" src="images/samples_symmetric_racgan.png" alt="comparison between AC-CT-GAN and rAC-CT-GAN"></a>
	  </p>
	  <p>
	    <a name="fig3-2"><img class="w-75" src="images/samples_symmetric_rcgan.png" alt="comparison between cSN-GAN and rcSN-GAN"></a>
	  </p>
	  <figcaption class="figure-caption text-left">
	    Figure 3. Generated image samples on CIFAR-10 (symmetric noise with a noise rate of 0.5). In each picture block, each column shows samples associated with the same class: <em>airplane</em>, <em>automobile</em>, <em>bird</em>, <em>cat</em>, <em>deer</em>, <em>dog</em>, <em>frog</em>, <em>horse</em>, <em>ship</em>, and <em>truck</em>, respectively, from left to right. Each row includes samples generated from a fixed <em><strong>z</strong></em> and a varied <em>y<sup>g</sup></em>.
	  </figcaption>
	</figure>

	<h3 class="text-center">Acknowledgment</h3>
	<p>
	  We would like to thank Hiroharu Kato, Yusuke Mukuta, and Mikihiro Tanaka for helpful discussions. This work was supported by JSPS KAKENHI Grant Number JP17H06100, partially supported by JST CREST Grant Number JPMJCR1403, Japan, and partially supported by the Ministry of Education, Culture, Sports, Science and Technology (MEXT) as "Seminal Issue on Post-K Computer."
	</p>

	<h3 class="text-center">Related work</h3>
	<p>
	  <strong>Note:</strong>
	  Kiran Koshy Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh published a paper <a href="#ref4">[4]</a> independently from us on the same problem. They use similar ideas as our work, albeit with a different architecture. You should also check out their awesome work at <a href="https://arxiv.org/abs/1811.03205">https://arxiv.org/abs/1811.03205</a>.
	</p>
	
	<p>
	  <a name="ref1" class="text-primary">[1]</a>
	  A. Odena, C. Olah, and J. Shlens.
	  <a href="https://arxiv.org/abs/1610.09585"><strong>Conditional Image Synthesis with Auxiliary Classifier GANs</strong></a>.
	  In ICML, 2017.<br>
	  
	  <a name="ref2" class="text-primary">[2]</a>
	  M. Mirza and S. Osindero.
	  <a href="https://arxiv.org/abs/1411.1784"><strong>Conditional Generative Adversarial Nets</strong></a>.
	  arXiv preprint arXiv:1411.1784, 2014.<br>
	  
	  <a name="ref3" class="text-primary">[3]</a>
	  T. Miyato and M. Koyama.
	  <a href="https://github.com/pfnet-research/sngan_projection"><strong>cGANs with Projection Discriminator</strong></a>.
	  In ICLR, 2018.<br>

	  <a name="ref4" class="text-primary">[4]</a>
	  K. K. Thekumparampil, A. Khetan, Z. Lin, and S. Oh.
	  <a href="https://github.com/POLane16/Robust-Conditional-GAN"><strong>Robustness of Conditional GANs to Noisy Labels</strong></a>.
	  In NeurIPS, 2018.<br>

	  <a name="ref5" class="text-primary">[5]</a>
	  T. Kaneko, Y. Ushiku, and T. Harada.
	  <a href="https://takuhirok.github.io/CP-GAN/"><strong>Class-Distinct and Class-Mutual Image Generation with GANs</strong></a>.
	  In BMVC, 2019.<br>

	  <a name="ref6" class="text-primary">[6]</a>
	  T. Kaneko and T. Harada.
	  <a href="https://takuhirok.github.io/NR-GAN/"><strong>Noise Robust Generative Adversarial Networks</strong></a>.
	  In CVPR, 2020.<br>

	  <a name="ref7" class="text-primary">[7]</a>
	  T. Kaneko and T. Harada.
	  <a href="https://takuhirok.github.io/BNCR-GAN/"><strong>Blur, Noise, and Compression Robust Generative Adversarial Networks</strong></a>.
	  In CVPR, 2021.
	</p>
      </div>
    </main>

    <footer class="text-center bg-whitesmoke">      
      <div class="container">
	<small>
	  <strong>Label-Noise Robust Generative Adversarial Networks</strong><br>
	  <a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/">Takuhiro Kaneko</a> | t.kaneko at mi.t.u-tokyo.ac.jp
	</small>
      </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>
