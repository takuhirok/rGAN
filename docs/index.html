<!doctype html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-138229553-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-138229553-1');
    </script>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <title>rGAN</title>
  </head>

  <style type="text/css">
    footer {
    padding-top: 10px;
    padding-bottom: 10px;
    }
    .bg-whitesmoke {
    background-color: whitesmoke
    }
    .thumbnail-shadow {
    filter: drop-shadow(5px 5px 5px #aaa);
    }
    table{
    margin: 0 auto
    }
  </style>
  
  <body>
    <header>
      <div class="jumbotron text-center bg-whitesmoke">
	<div class="container">
	  <h2>Label-Noise Robust Generative Adversarial Networks</h2>
	  <p class="lead">
	    <a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/">Takuhiro Kaneko</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
	    <a href="https://yoshitakaushiku.net/">Yoshitaka Ushiku</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
	    <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada</a><sup>1,2</sup>&nbsp;&nbsp;&nbsp;<br>
	    <sup>1</sup>The University of Tokyo&nbsp;&nbsp;&nbsp;
	    <sup>2</sup>RIKEN
	  </p>
	  <p class="lead">
	  CVPR 2019 (Oral)<br>
	  <a href="https://arxiv.org/abs/1811.11165">[Paper]</a>
	  <a href="https://github.com/takuhirok/rGAN/">[Code]</a>
	  </p>
	</div>
      </div>
    </header>

    <main>
      <div class="container">
	<figure class="figure text-center">
	  <p>
	    <a name="fig1"><img class="w-100" src="images/examples.png" alt="examples"></a>
	  </p>
	  <figcaption class="figure-caption text-left">
	    Figure 1. Examples of label-noise robust conditional image generation. rGAN can learn a label-noise robust conditional generator that can generate an image conditioned on the <em>clean</em> label rather than conditioned on the <em>noisy</em> label even when the <em>noisy</em> labeled data are only available during the training.
	  </figcaption>
	</figure>

	<h3 class="text-center">Abstract</h3>
	<p>
	  Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class-conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called <strong>label-noise robust GANs (rGANs)</strong>, which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: <strong>rAC-GAN</strong>, which is a bridging model between AC-GAN and the label-noise robust classification model, and <strong>rcGAN</strong>, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).
	</p>
	
	<h3 class="text-center">Paper</h3>
	<p>
	  <table>
	    <tbody>
	      <tr>
		<td>
		  <a href="https://arxiv.org/abs/1811.11165"><img class="thumbnail-shadow" alt="paper thumbnail" src="images/paper_thumbnail.png"  width=150></a>
		</td>
		<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
		<td class="text-center">
		  <a href="https://arxiv.org/abs/1811.11165" class="lead">[Paper]</a><br>
		  arXiv:1811.11165<br>Nov. 2018.
		</td>
	      </tr>
	    </tbody>
	  </table>
	</p>
	
	<h4 class="text-center">Citation</h4>
	<p class="text-center">
	  Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada.<br>
	  Label-Noise Robust Generative Adversarial Networks. In CVPR, 2019.<br>
	  <a href="rGAN.txt" class="lead">[BibTex]</a>
	</p>

	<h3 class="text-center">Code</h3>
	<p class="text-center lead">
	  <a href="https://github.com/takuhirok/rGAN/">[PyTorch]</a>
	</p>
	
	<h3 class="text-center">Overview</h3>
	<p>
	  Our task is, when given <em>noisy labeled</em> data, to construct a label-noise robust conditional generator that can generate an image conditioned on the <em>clean label</em> rather than conditioned on the <em>noisy label</em>. Our main idea for solving this problem is to incorporate a <strong>noise transition model</strong> (viewed as orange rectangles in Figures 2(b) and (d); which represents a probability that a clean label is corrupted to a noisy label) into typical class-conditional GANs. In particular, we develop two variants: <strong>rAC-GAN</strong> (<a href="#fig2">Figure 2(b)</a>) and <strong>rcGAN</strong> (<a href="#fig2">Figure 2(d)</a>) that are extensions of AC-GAN <a href="#ref1">[1]</a> (<a href="#fig2">Figure 2(a)</a>) and cGAN <a href="#ref2">[2]</a> <a href="#ref3">[3]</a> (<a href="#fig2">Figure 2(c)</a>), respectively.
	</p>
	
	<figure class="figure text-center">
	  <p>
	    <a name="fig2"><img class="w-100" src="images/networks.png" alt="examples"></a>
	  </p>
	  <figcaption class="figure-caption text-left">
	    Figure 2. Comparison of naive and label-noise robust GANs. We denote the discriminator and auxiliary classifier by <em>D</em> and <em>C</em>, respectively. In our rAC-GAN (b) and rcGAN (d), we incorporate a noise transition model (viewed as an orange rectangle) into AC-GAN (a) and cGAN (c), respectively.
	  </figcaption>
	</figure>

	<h3 class="text-center">Samples</h3>
	<p class="text-center">
	  CIFAR-10 (symmetric noise with a noise rate of 0.5)
	</p>
	<figure class="figure text-center">
	  <p>
	    <a name="fig3-1"><img class="w-75" src="images/samples_symmetric_racgan.png" alt="comparison between AC-CT-GAN and rAC-CT-GAN"></a>
	  </p>
	  <p>
	    <a name="fig3-2"><img class="w-75" src="images/samples_symmetric_rcgan.png" alt="comparison between cSN-GAN and rcSN-GAN"></a>
	  </p>
	  <figcaption class="figure-caption text-left">
	    Figure 3. Generated image samples on CIFAR-10 (symmetric noise with a noise rate of 0.5). In each picture block, each column shows samples associated with the same class: <em>airplane</em>, <em>automobile</em>, <em>bird</em>, <em>cat</em>, <em>deer</em>, <em>dog</em>, <em>frog</em>, <em>horse</em>, <em>ship</em>, and <em>truck</em>, respectively, from left to right. Each row includes samples generated from a fixed <em><strong>z</strong></em> and a varied <em>y<sup>g</sup></em>.
	  </figcaption>
	</figure>

	<h3 class="text-center">Acknowledgement</h3>
	<p>
	  We would like to thank Hiroharu Kato, Yusuke Mukuta, and Mikihiro Tanaka for helpful discussions. This work was supported by JSPS KAKENHI Grant Number JP17H06100, partially supported by JST CREST Grant Number JPMJCR1403, Japan, and partially supported by the Ministry of Education, Culture, Sports, Science and Technology (MEXT) as "Seminal Issue on Post-K Computer."
	</p>

	<h3 class="text-center">Related work</h3>
	<p>
	  <a name="ref1" class="text-primary">[1]</a>
	  A. Odena, C. Olah, and J. Shlens.
	  Conditional image synthesis with auxiliary classifier GANs.
	  In ICML, 2017.
	  <a href="https://arxiv.org/abs/1610.09585">[arXiv]</a><br>
	  
	  <a name="ref2" class="text-primary">[2]</a>
	  M. Mirza and S. Osindero.
	  Conditional generative adversarial nets.
	  arXiv preprint arXiv:1411.1784, 2014.
	  <a href="https://arxiv.org/abs/1411.1784">[arXiv]</a><br>
	  
	  <a name="ref3" class="text-primary">[3]</a>
	  T. Miyato and M. Koyama.
	  cGANs with projection discriminator.
	  In ICLR, 2018.
	  <a href="https://arxiv.org/abs/1802.05637">[arXiv]</a>
	</p>	
      </div>
    </main>

    <footer class="text-center bg-whitesmoke">      
      <div class="container">
	<small>
	  <strong>Label-Noise Robust Generative Adversarial Networks</strong><br>
	  <a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/">Takuhiro Kaneko</a> | t.kaneko at mi.t.u-tokyo.ac.jp
	</small>
      </div>
    </footer>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>
